{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy without scaling: 1.0\n",
      "Accuracy with scaling: 0.3157894736842105\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0           0.222222          0.625000           0.067797          0.041667\n",
      "1           0.166667          0.416667           0.067797          0.041667\n",
      "2           0.111111          0.500000           0.050847          0.041667\n",
      "3           0.083333          0.458333           0.084746          0.041667\n",
      "4           0.194444          0.666667           0.067797          0.041667\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0          -0.900681          1.019004          -1.340227         -1.315444\n",
      "1          -1.143017         -0.131979          -1.340227         -1.315444\n",
      "2          -1.385353          0.328414          -1.397064         -1.315444\n",
      "3          -1.506521          0.098217          -1.283389         -1.315444\n",
      "4          -1.021849          1.249201          -1.340227         -1.315444\n",
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0          -0.538462               1.0          -0.842857         -0.733333\n",
      "1          -0.692308               0.0          -0.842857         -0.733333\n",
      "2          -0.846154               0.4          -0.871429         -0.733333\n",
      "3          -0.923077               0.2          -0.814286         -0.733333\n",
      "4          -0.615385               1.2          -0.842857         -0.733333\n"
     ]
    }
   ],
   "source": [
    "# Question 1: Feature Scaling\n",
    "# Task: Explain why feature scaling is essential and demonstrate the impact of unscaled features on a machine learning model.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "X, y = load_iris(return_X_y=True)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=42)\n",
    "model_unscaled = LogisticRegression(max_iter=1000)\n",
    "model_unscaled.fit(X_train, y_train)\n",
    "acc_unscaled = accuracy_score(y_test, model_unscaled.predict(X_test))\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "model_scaled = LogisticRegression(max_iter=1000)\n",
    "model_scaled.fit(X_train_scaled, y_train)\n",
    "acc_scaled = accuracy_score(y_test, model_scaled.predict(X_test))\n",
    "print(\"Accuracy without scaling:\", acc_unscaled)\n",
    "print(\"Accuracy with scaling:\", acc_scaled)\n",
    "\n",
    "\n",
    "# Question 2: Min-Max Scaling\n",
    "# Task: Implement Min-Max Scaling on the Iris dataset.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "X = iris.data \n",
    "feature_names = iris.feature_names\n",
    "df = pd.DataFrame(X, columns=feature_names)\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=feature_names)\n",
    "print(df_scaled.head())\n",
    "\n",
    "# Question 3: Standardization (Z-score Scaling)\n",
    "# Task: Implement Standardization using Z-score scaling on the Iris dataset.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target \n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "df_scaled = pd.DataFrame(X_scaled, columns=iris.feature_names)\n",
    "print(df_scaled.head())\n",
    "\n",
    "\n",
    "# Question 4: Robust Scaling\n",
    "# Task: Implement Robust Scaling to handle outliers in the Iris dataset.\n",
    "\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import pandas as pd\n",
    "iris = load_iris()\n",
    "X = iris.data  \n",
    "y = iris.target\n",
    "df = pd.DataFrame(X, columns=iris.feature_names)\n",
    "scaler = RobustScaler()\n",
    "X_robust_scaled = scaler.fit_transform(X)\n",
    "df_robust_scaled = pd.DataFrame(X_robust_scaled, columns=iris.feature_names)\n",
    "print(df_robust_scaled.head())\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
